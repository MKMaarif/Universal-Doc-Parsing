{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torchmetrics jiwer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read json\n",
    "with open('res/benchmark_data.json') as f:\n",
    "    benchmark_data = json.load(f)\n",
    " \n",
    "# benchmark_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Vidavox\\Universal Doc Parsing\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Character Error rate (CER)\n",
    "from torchmetrics.text import CharErrorRate\n",
    "import jiwer\n",
    "\n",
    "\n",
    "cer = CharErrorRate()\n",
    "\n",
    "transforms = jiwer.Compose(\n",
    "    [\n",
    "        jiwer.ExpandCommonEnglishContractions(),\n",
    "        jiwer.RemoveEmptyStrings(),\n",
    "        jiwer.ToLowerCase(),\n",
    "        jiwer.RemoveMultipleSpaces(),\n",
    "        jiwer.Strip(),\n",
    "        jiwer.RemovePunctuation(),\n",
    "        jiwer.ReduceToListOfListOfWords(),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# benchmarking function\n",
    "def benchmark(benchmark_data, parser_res):\n",
    "    cer_scores = []\n",
    "    wer_scores = []\n",
    "    for i, page in enumerate(benchmark_data['pages']):\n",
    "        print(f\"Page {i+1}\")\n",
    "        target = page['markdown']\n",
    "        preds = parser_res['pages'][i]['markdown']\n",
    "\n",
    "        # cer score\n",
    "        cer_score = cer(preds=preds, target=target).item()\n",
    "\n",
    "        # wer score\n",
    "        wer_score = jiwer.wer(reference=target, hypothesis=preds, truth_transform=transforms, hypothesis_transform=transforms)\n",
    "\n",
    "        cer_scores.append(round(cer_score, 4))\n",
    "        wer_scores.append(round(wer_score, 4))\n",
    "\n",
    "        print(f\"CER: {cer_score:.4f}\")\n",
    "        print(f\"WER: {wer_score:.4f}\")\n",
    "        print()\n",
    "    return cer_scores, wer_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LlamaParse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read json\n",
    "with open('res/llamaparse_res.json') as f:\n",
    "    llamaparse_res = json.load(f)\n",
    " \n",
    "# llamaparse_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1\n",
      "CER: 0.0122\n",
      "WER: 0.0099\n",
      "\n",
      "Page 2\n",
      "CER: 0.0006\n",
      "WER: 0.0014\n",
      "\n",
      "Page 3\n",
      "CER: 0.6014\n",
      "WER: 0.5201\n",
      "\n",
      "Page 4\n",
      "CER: 0.0163\n",
      "WER: 0.0030\n",
      "\n",
      "Page 5\n",
      "CER: 0.6048\n",
      "WER: 0.6636\n",
      "\n",
      "Page 6\n",
      "CER: 0.0059\n",
      "WER: 0.0167\n",
      "\n",
      "CER List: [0.0122, 0.0006, 0.6014, 0.0163, 0.6048, 0.0059]\n",
      "WER List: [0.0099, 0.0014, 0.5201, 0.003, 0.6636, 0.0167]\n",
      "Average CER: 0.2069\n",
      "Average WER: 0.2024\n"
     ]
    }
   ],
   "source": [
    "# benchmarking llamaparse\n",
    "llamaparse_cer, llamaparse_wer = benchmark(benchmark_data, llamaparse_res)\n",
    "\n",
    "print(f\"CER List: {llamaparse_cer}\")\n",
    "print(f\"WER List: {llamaparse_wer}\")\n",
    "print(f\"Average CER: {sum(llamaparse_cer)/len(llamaparse_cer):.4f}\")\n",
    "print(f\"Average WER: {sum(llamaparse_wer)/len(llamaparse_wer):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mistral OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read json\n",
    "with open('res/mistralocr_res.json') as f:\n",
    "    mistralocr_res = json.load(f)\n",
    " \n",
    "# mistralocr_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1\n",
      "CER: 0.2643\n",
      "WER: 0.2184\n",
      "\n",
      "Page 2\n",
      "CER: 0.1322\n",
      "WER: 0.1580\n",
      "\n",
      "Page 3\n",
      "CER: 0.2798\n",
      "WER: 0.2488\n",
      "\n",
      "Page 4\n",
      "CER: 0.0442\n",
      "WER: 0.0838\n",
      "\n",
      "Page 5\n",
      "CER: 0.1493\n",
      "WER: 0.3822\n",
      "\n",
      "Page 6\n",
      "CER: 0.6077\n",
      "WER: 0.6517\n",
      "\n",
      "CER List: [0.2643, 0.1322, 0.2798, 0.0442, 0.1493, 0.6077]\n",
      "WER List: [0.2184, 0.158, 0.2488, 0.0838, 0.3822, 0.6517]\n",
      "Average CER: 0.2462\n",
      "Average WER: 0.2905\n"
     ]
    }
   ],
   "source": [
    "# benchmarking mistralocr\n",
    "mistralocr_cer, mistralocr_wer = benchmark(benchmark_data, mistralocr_res)\n",
    "\n",
    "print(f\"CER List: {mistralocr_cer}\")\n",
    "print(f\"WER List: {mistralocr_wer}\")\n",
    "print(f\"Average CER: {sum(mistralocr_cer)/len(mistralocr_cer):.4f}\")\n",
    "print(f\"Average WER: {sum(mistralocr_wer)/len(mistralocr_wer):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline 1 (Yolo + GOT-OCR2 + Gemma 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read json\n",
    "with open('res/pipeline_1_res.json') as f:\n",
    "    pipeline_1_res = json.load(f)\n",
    " \n",
    "# pipeline_1_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1\n",
      "CER: 0.1884\n",
      "WER: 0.2159\n",
      "\n",
      "Page 2\n",
      "CER: 0.3745\n",
      "WER: 0.5091\n",
      "\n",
      "Page 3\n",
      "CER: 0.2224\n",
      "WER: 0.2809\n",
      "\n",
      "Page 4\n",
      "CER: 0.4662\n",
      "WER: 0.4656\n",
      "\n",
      "Page 5\n",
      "CER: 0.6646\n",
      "WER: 0.7483\n",
      "\n",
      "Page 6\n",
      "CER: 0.9830\n",
      "WER: 0.9950\n",
      "\n",
      "CER List: [0.1884, 0.3745, 0.2224, 0.4662, 0.6646, 0.983]\n",
      "WER List: [0.2159, 0.5091, 0.2809, 0.4656, 0.7483, 0.995]\n",
      "Average CER: 0.4832\n",
      "Average WER: 0.5358\n"
     ]
    }
   ],
   "source": [
    "pipeline_1_cer, pipeline_1_wer = benchmark(benchmark_data, pipeline_1_res)\n",
    "\n",
    "print(f\"CER List: {pipeline_1_cer}\")\n",
    "print(f\"WER List: {pipeline_1_wer}\")\n",
    "print(f\"Average CER: {sum(pipeline_1_cer)/len(pipeline_1_cer):.4f}\")\n",
    "print(f\"Average WER: {sum(pipeline_1_wer)/len(pipeline_1_wer):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline 2 (Yolo + Universal.io + Gemma 3 + Gemma 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read json\n",
    "with open('res/pipeline_2_res.json') as f:\n",
    "    pipeline_2_res = json.load(f)\n",
    " \n",
    "# pipeline_2_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1\n",
      "CER: 0.2914\n",
      "WER: 0.2978\n",
      "\n",
      "Page 2\n",
      "CER: 0.1032\n",
      "WER: 0.0657\n",
      "\n",
      "Page 3\n",
      "CER: 0.2228\n",
      "WER: 0.1862\n",
      "\n",
      "Page 4\n",
      "CER: 0.3530\n",
      "WER: 0.3039\n",
      "\n",
      "Page 5\n",
      "CER: 0.6337\n",
      "WER: 0.7918\n",
      "\n",
      "Page 6\n",
      "CER: 0.2132\n",
      "WER: 0.1900\n",
      "\n",
      "CER List: [0.2914, 0.1032, 0.2228, 0.353, 0.6337, 0.2132]\n",
      "WER List: [0.2978, 0.0657, 0.1862, 0.3039, 0.7918, 0.19]\n",
      "Average CER: 0.3029\n",
      "Average WER: 0.3059\n"
     ]
    }
   ],
   "source": [
    "pipeline_2_cer, pipeline_2_wer = benchmark(benchmark_data, pipeline_2_res)\n",
    "\n",
    "print(f\"CER List: {pipeline_2_cer}\")\n",
    "print(f\"WER List: {pipeline_2_wer}\")\n",
    "print(f\"Average CER: {sum(pipeline_2_cer)/len(pipeline_2_cer):.4f}\")\n",
    "print(f\"Average WER: {sum(pipeline_2_wer)/len(pipeline_2_wer):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline 3 (Yolo + Universal.io + Gemma 3 + phi-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('json_res/pipeline_2_res.jsonl') as f:\n",
    "#     pipeline_2_res = json.load(f)\n",
    "\n",
    "# pipeline_2_cer, pipeline_2_wer = benchmark_text(benchmark_data, pipeline_2_res)\n",
    "\n",
    "# print(f\"CER List: {pipeline_2_cer}\")\n",
    "# print(f\"WER List: {pipeline_2_wer}\")\n",
    "# print()\n",
    "# print(f\"Average CER: {sum(pipeline_2_cer)/len(pipeline_2_cer):.4f}\")\n",
    "# print(f\"Average WER: {sum(pipeline_2_wer)/len(pipeline_2_wer):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
