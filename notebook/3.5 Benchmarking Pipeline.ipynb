{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Requrements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read json file\n",
    "with open('json_res/benchmark_data.jsonl') as f:\n",
    "    benchmark_data = json.load(f)\n",
    "\n",
    "with open('json_res/llamaparse_res.jsonl') as f:\n",
    "    llamaparse_res = json.load(f)\n",
    "\n",
    "with open('json_res/mistralocr_res.jsonl') as f:\n",
    "    mistralocr_res = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchmetrics in e:\\vidavox\\universal doc parsing\\.venv\\lib\\site-packages (1.6.3)\n",
      "Requirement already satisfied: jiwer in e:\\vidavox\\universal doc parsing\\.venv\\lib\\site-packages (3.1.0)\n",
      "Requirement already satisfied: numpy>1.20.0 in e:\\vidavox\\universal doc parsing\\.venv\\lib\\site-packages (from torchmetrics) (2.2.3)\n",
      "Requirement already satisfied: packaging>17.1 in e:\\vidavox\\universal doc parsing\\.venv\\lib\\site-packages (from torchmetrics) (24.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in e:\\vidavox\\universal doc parsing\\.venv\\lib\\site-packages (from torchmetrics) (2.6.0)\n",
      "Requirement already satisfied: lightning-utilities>=0.8.0 in e:\\vidavox\\universal doc parsing\\.venv\\lib\\site-packages (from torchmetrics) (0.14.1)\n",
      "Requirement already satisfied: click>=8.1.8 in e:\\vidavox\\universal doc parsing\\.venv\\lib\\site-packages (from jiwer) (8.1.8)\n",
      "Requirement already satisfied: rapidfuzz>=3.9.7 in e:\\vidavox\\universal doc parsing\\.venv\\lib\\site-packages (from jiwer) (3.12.2)\n",
      "Requirement already satisfied: colorama in e:\\vidavox\\universal doc parsing\\.venv\\lib\\site-packages (from click>=8.1.8->jiwer) (0.4.6)\n",
      "Requirement already satisfied: setuptools in e:\\vidavox\\universal doc parsing\\.venv\\lib\\site-packages (from lightning-utilities>=0.8.0->torchmetrics) (65.5.0)\n",
      "Requirement already satisfied: typing_extensions in e:\\vidavox\\universal doc parsing\\.venv\\lib\\site-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.12.2)\n",
      "Requirement already satisfied: filelock in e:\\vidavox\\universal doc parsing\\.venv\\lib\\site-packages (from torch>=2.0.0->torchmetrics) (3.18.0)\n",
      "Requirement already satisfied: networkx in e:\\vidavox\\universal doc parsing\\.venv\\lib\\site-packages (from torch>=2.0.0->torchmetrics) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in e:\\vidavox\\universal doc parsing\\.venv\\lib\\site-packages (from torch>=2.0.0->torchmetrics) (3.1.6)\n",
      "Requirement already satisfied: fsspec in e:\\vidavox\\universal doc parsing\\.venv\\lib\\site-packages (from torch>=2.0.0->torchmetrics) (2025.3.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in e:\\vidavox\\universal doc parsing\\.venv\\lib\\site-packages (from torch>=2.0.0->torchmetrics) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in e:\\vidavox\\universal doc parsing\\.venv\\lib\\site-packages (from sympy==1.13.1->torch>=2.0.0->torchmetrics) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in e:\\vidavox\\universal doc parsing\\.venv\\lib\\site-packages (from jinja2->torch>=2.0.0->torchmetrics) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install torchmetrics jiwer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Vidavox\\Universal Doc Parsing\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Character Error rate (CER)\n",
    "from torchmetrics.text import CharErrorRate\n",
    "import jiwer\n",
    "\n",
    "\n",
    "cer = CharErrorRate()\n",
    "\n",
    "transforms = jiwer.Compose(\n",
    "    [\n",
    "        jiwer.ExpandCommonEnglishContractions(),\n",
    "        jiwer.RemoveEmptyStrings(),\n",
    "        jiwer.ToLowerCase(),\n",
    "        jiwer.RemoveMultipleSpaces(),\n",
    "        jiwer.Strip(),\n",
    "        jiwer.RemovePunctuation(),\n",
    "        jiwer.ReduceToListOfListOfWords(),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# benchmarking function\n",
    "def benchmark(benchmark_data, parser_res):\n",
    "    cer_scores = []\n",
    "    wer_scores = []\n",
    "    for i, page in enumerate(benchmark_data['pages']):\n",
    "        print(f\"Page {i+1}\")\n",
    "        target = page['markdown']\n",
    "        preds = parser_res['pages'][i]['markdown']\n",
    "\n",
    "        # cer score\n",
    "        cer_score = cer(preds=preds, target=target).item()\n",
    "\n",
    "        # wer score\n",
    "        wer_score = jiwer.wer(reference=target, hypothesis=preds, truth_transform=transforms, hypothesis_transform=transforms)\n",
    "\n",
    "        cer_scores.append(cer_score)\n",
    "        wer_scores.append(wer_score)\n",
    "\n",
    "        print(f\"CER: {cer_score:.4f}\")\n",
    "        print(f\"WER: {wer_score:.4f}\")\n",
    "        print()\n",
    "    return cer_scores, wer_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# benchmarking function\n",
    "def benchmark_text(benchmark_data, parser_res):\n",
    "    cer_scores = []\n",
    "    wer_scores = []\n",
    "    for i, page in enumerate(benchmark_data['pages']):\n",
    "        print(f\"Page {i+1}\")\n",
    "        target = page['markdown']\n",
    "        preds = parser_res['pages'][i]['text']\n",
    "\n",
    "        # cer score\n",
    "        cer_score = cer(preds=preds, target=target).item()\n",
    "\n",
    "        # wer score\n",
    "        wer_score = jiwer.wer(reference=target, hypothesis=preds, truth_transform=transforms, hypothesis_transform=transforms)\n",
    "\n",
    "        cer_scores.append(cer_score)\n",
    "        wer_scores.append(wer_score)\n",
    "\n",
    "        print(f\"CER: {cer_score:.4f}\")\n",
    "        print(f\"WER: {wer_score:.4f}\")\n",
    "        print()\n",
    "    return cer_scores, wer_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LlamaParse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1\n",
      "CER: 0.0111\n",
      "WER: 0.0074\n",
      "\n",
      "Page 2\n",
      "CER: 0.0015\n",
      "WER: 0.0000\n",
      "\n",
      "Page 3\n",
      "CER: 0.6966\n",
      "WER: 0.5180\n",
      "\n",
      "CER List: [0.011148648336529732, 0.0015455950051546097, 0.6966218948364258]\n",
      "WER List: [0.007444168734491315, 0.0, 0.5180327868852459]\n",
      "\n",
      "Average CER: 0.2364\n",
      "Average WER: 0.1752\n"
     ]
    }
   ],
   "source": [
    "# benchmarking llamaparse\n",
    "llamaparse_cer, llamaparse_wer = benchmark(benchmark_data, llamaparse_res)\n",
    "\n",
    "print(f\"CER List: {llamaparse_cer}\")\n",
    "print(f\"WER List: {llamaparse_wer}\")\n",
    "print()\n",
    "print(f\"Average CER: {sum(llamaparse_cer)/len(llamaparse_cer):.4f}\")\n",
    "print(f\"Average WER: {sum(llamaparse_wer)/len(llamaparse_wer):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mistral OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1\n",
      "CER: 0.2666\n",
      "WER: 0.2184\n",
      "\n",
      "Page 2\n",
      "CER: 0.1339\n",
      "WER: 0.1566\n",
      "\n",
      "Page 3\n",
      "CER: 0.2679\n",
      "WER: 0.2328\n",
      "\n",
      "CER List: [0.266554057598114, 0.1338871717453003, 0.2678648829460144]\n",
      "WER List: [0.21836228287841192, 0.15664335664335666, 0.23278688524590163]\n",
      "\n",
      "Average CER: 0.2228\n",
      "Average WER: 0.2026\n"
     ]
    }
   ],
   "source": [
    "# benchmarking mistralocr\n",
    "mistralocr_cer, mistralocr_wer = benchmark(benchmark_data, mistralocr_res)\n",
    "\n",
    "print(f\"CER List: {mistralocr_cer}\")\n",
    "print(f\"WER List: {mistralocr_wer}\")\n",
    "print()\n",
    "print(f\"Average CER: {sum(mistralocr_cer)/len(mistralocr_cer):.4f}\")\n",
    "print(f\"Average WER: {sum(mistralocr_wer)/len(mistralocr_wer):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline 1 (Yolo + GOT-OCR2 + Gemma 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1\n",
      "CER: 0.1932\n",
      "WER: 0.2035\n",
      "\n",
      "Page 2\n",
      "CER: 0.3752\n",
      "WER: 0.5091\n",
      "\n",
      "Page 3\n",
      "CER: 0.2150\n",
      "WER: 0.2902\n",
      "\n",
      "CER List: [0.19324325025081635, 0.37519320845603943, 0.21502815186977386]\n",
      "WER List: [0.20347394540942929, 0.509090909090909, 0.2901639344262295]\n",
      "\n",
      "Average CER: 0.2612\n",
      "Average WER: 0.3342\n"
     ]
    }
   ],
   "source": [
    "with open('json_res/pipeline_1_res.jsonl') as f:\n",
    "    pipeline_1_res = json.load(f)\n",
    "\n",
    "pipeline_1_cer, pipeline_1_wer = benchmark(benchmark_data, pipeline_1_res)\n",
    "\n",
    "print(f\"CER List: {pipeline_1_cer}\")\n",
    "print(f\"WER List: {pipeline_1_wer}\")\n",
    "print()\n",
    "print(f\"Average CER: {sum(pipeline_1_cer)/len(pipeline_1_cer):.4f}\")\n",
    "print(f\"Average WER: {sum(pipeline_1_wer)/len(pipeline_1_wer):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline 2 (Yolo + Universal.io + Gemma 3) - With Formating VLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1\n",
      "CER: 0.3922\n",
      "WER: 0.3970\n",
      "\n",
      "Page 2\n",
      "CER: 0.0330\n",
      "WER: 0.0392\n",
      "\n",
      "Page 3\n",
      "CER: 0.8010\n",
      "WER: 0.9000\n",
      "\n",
      "CER List: [0.39222973585128784, 0.03303709253668785, 0.8009961247444153]\n",
      "WER List: [0.3970223325062035, 0.039160839160839164, 0.9]\n",
      "\n",
      "Average CER: 0.4088\n",
      "Average WER: 0.4454\n"
     ]
    }
   ],
   "source": [
    "with open('json_res/pipeline_2_res.jsonl') as f:\n",
    "    pipeline_2_res = json.load(f)\n",
    "\n",
    "pipeline_2_cer, pipeline_2_wer = benchmark(benchmark_data, pipeline_2_res)\n",
    "\n",
    "print(f\"CER List: {pipeline_2_cer}\")\n",
    "print(f\"WER List: {pipeline_2_wer}\")\n",
    "print()\n",
    "print(f\"Average CER: {sum(pipeline_2_cer)/len(pipeline_2_cer):.4f}\")\n",
    "print(f\"Average WER: {sum(pipeline_2_wer)/len(pipeline_2_wer):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline 2 (Yolo + Universal.io + Gemma 3) - Only Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1\n",
      "CER: 0.2608\n",
      "WER: 0.2779\n",
      "\n",
      "Page 2\n",
      "CER: 0.0195\n",
      "WER: 0.0587\n",
      "\n",
      "Page 3\n",
      "CER: 0.1849\n",
      "WER: 0.1459\n",
      "\n",
      "CER List: [0.26081082224845886, 0.019513137638568878, 0.1849285364151001]\n",
      "WER List: [0.27791563275434245, 0.05874125874125874, 0.14590163934426228]\n",
      "\n",
      "Average CER: 0.1551\n",
      "Average WER: 0.1609\n"
     ]
    }
   ],
   "source": [
    "with open('json_res/pipeline_2_res.jsonl') as f:\n",
    "    pipeline_2_res = json.load(f)\n",
    "\n",
    "pipeline_2_cer, pipeline_2_wer = benchmark_text(benchmark_data, pipeline_2_res)\n",
    "\n",
    "print(f\"CER List: {pipeline_2_cer}\")\n",
    "print(f\"WER List: {pipeline_2_wer}\")\n",
    "print()\n",
    "print(f\"Average CER: {sum(pipeline_2_cer)/len(pipeline_2_cer):.4f}\")\n",
    "print(f\"Average WER: {sum(pipeline_2_wer)/len(pipeline_2_wer):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
