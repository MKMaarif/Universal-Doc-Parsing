{
    "source": "attention_paper.pdf",
    "pages": [
        {
            "index": 0,
            "markdown": "# Attention Is All You Need\n\n## Authors\n- Aditya Narwal\n- Noah Jang\n- Nilu Fernando\n- Jakob Uhlig\n- Linnea Jones\n- Adrian Gonzalez\n- Lukas Kaiser\n- Bill Piskorz\n\n## Abstract\nThe dominant sequence transduction models are based on complex recurrent neural networks (RNNs) with long-term dependencies. These models are also connected to the encoder and decoder through an attention mechanism. Experiments on two machine translation tasks show that these models outperform the recurrent neural network (RNN) baseline and the encoder-decoder RNN baseline. The training of these models is much faster than the RNN baseline, and the performance is comparable to the encoder-decoder RNN baseline. The training of these models is much faster than the RNN baseline, and the performance is comparable to the encoder-decoder RNN baseline.\n\n## Target Contributions\n- Using meta-learning to improve RNN with self-attention and scaled dot-product attention.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder RNN baseline.\n- Improving the performance of the encoder-decoder baseline.\n- Improving the performance of the encoder-decoder baseline.\n- Improving the performance of the encoder-decoder baseline.\n- Improving the performance of the encoder-decoder baseline.\n- Improving the performance of the encoder-decoder baseline.\n- Improving the performance of the encoder-decoder baseline.\n- Improving the performance of the encoder-decoder.\n- Improving the performance of the encoder-decoder baseline.\n- Improving the performance of the encoder-decoder.\n- Improving the performance of the encoder-decoder.\n- Improving the performance of the encoder-decoder.\n- Improving the performance of the encoder-decoder.\n-improving the performance of the encoder-decoder.\n- Improving the performance of the encoder-decoder, and-imping the performance of the performance of the time.\n- Implying the performance of the performing the time.\n-imaging.\n- Impeaking.\n- Impt.\n- and improving the performance of the time.\n-imaging.\n-"
        },
        {
            "index": 1,
            "markdown": "# Introduction\n\nRecurrent neural networks, long short-term memory (LSTM) and gated recurrent units (GRU) are widely used in natural language processing (NLP) tasks. These models are capable of capturing long-range dependencies in sequential data, which is crucial for tasks such as language modeling and machine translation. However, the training of these models can be challenging due to the vanishing gradient problem, which limits the ability of the model to learn long-term dependencies.\n\nRecent research has focused on developing architectures that can overcome this limitation. One such approach is the Transformer, which was introduced in the paper \"Attention is All You Need\" by Vaswani et al. [1]. The Transformer architecture relies on the self-attention mechanism, which allows the model to weigh the importance of different parts of the input sequence. This mechanism enables the model to capture long-range dependencies more effectively than traditional recurrent neural networks.\n\nIn this work, we propose the Transformer, a novel architecture combining recurrence and mutual information. The Transformer is based on the Transformer architecture, but with a modified recurrence mechanism that incorporates mutual information. We demonstrate that the Transformer can achieve state-of-the-art performance on several NLP tasks, including machine translation quality after being trained for a little as twelve hours on eight NVIDIA GPUs.\n\n## 2 Background\n\nRecurrent neural networks (RNNs) and their variants, such as Long Short-Term Memory (LSTM) and Gated Recurrent Units (GRU), have been widely used in natural language processing (NLP) tasks. These models are capable of capturing long-range dependencies in sequential data, which is crucial for tasks such as language modeling and machine translation. However, the training of these models can be challenging due to the vanishing gradient problem, which limits the ability of the model to learn long-term dependencies.\n\nRecent research has focused on developing architectures that can overcome this limitation. One such approach is the Transformer, which was introduced in the paper \"Attention is All You Need\" by Vaswani et al. [1]. The Transformer architecture relies on the self-attention mechanism, which allows the model to weigh the importance of different parts of the input sequence. This mechanism enables the model to capture long-range dependencies more effectively than traditional recurrent neural networks.\n\nIn this work, we propose the Transformer, a novel architecture combining recurrence and mutual information. The Transformer is based on the Transformer architecture, but with a modified recurrence mechanism that incorporates mutual information. We demonstrate that the Transformer can achieve state-of-the-art performance on several NLP tasks, including machine translation quality after being trained for a little as twelve hours on eight NVIDIA GPUs.\n\n## 3 Model Architecture\n\nMost competitive neural sequence transduction models have an encoder-decoder structure. The encoder is typically a recurrent neural network (RNN) or a transformer, which processes the input sequence and generates a sequence of hidden states. The decoder is also typically an RNN or a transformer, which generates the output sequence one token at a time.\n\nIn this work, we propose the Translator, a novel architecture that combines recurrence and mutual information. The Translator is based on the Transformer architecture, but with a modified recurrence mechanism that incorporates mutual information. The Translator consists of an encoder and a decoder. The encoder is a Transformer that takes the input sequence and generates a sequence of hidden states. The decoder is also a Transformer that generates the output sequence one token at a time.\n\nThe Translator is trained using a masked language modeling objective, which is similar to the original Transformer. However, the Translator also incorporates a masked self-attention mechanism, which allows the model to learn to predict the masked tokens in the input sequence. The Translator is trained using a masked language modeling objective, which is similar to the original Transformer. However, the Translator also incorporates a masked self-attention mechanism, which allows the model to learn to predict the masked tokens in the input sequence.\n\n## 4 Conclusion\n\nIn this work, we propose the Translator, a novel architecture that combines recurrence and mutual information. The Translator is based on the Transformer architecture, but with a modified recurrence mechanism that incorporates mutual information. We demonstrate that the Translator can achieve state-of-the-art performance on several NLP tasks, including machine translation quality after being trained for a little as twelve hours on eight NVIDIA GPUs.\n\n## 5 References\n\n[1] Vaswani et al. \"Attention is All You Need\" [2017]\n"
        },
        {
            "index": 2,
            "markdown": "# The Transformer: model architecture\n\n## Figure 1: The Transformer: model architecture\n\n### Encoder and Decoder Stacks\n\n### Encoder\n\nThe encoder is composed of a stack \\( V \\times S \\) of identical layers. Each layer has two sub-layers:\n\n1. **Positional Encoding**: This sub-layer adds positional information to the input embeddings.\n2. **Multi-Head Self-Attention**: This sub-layer performs multi-head self-attention to capture dependencies between the input embeddings.\n\nThe output of each sub-layer is passed through a linear transformation and a ReLU activation function.\n\n### Decoder\n\nThe decoder is composed of a stack \\( V \\times S \\) of identical layers. Each layer has two sub-layers:\n\n1. **Positional Encoding**: This sub-layer adds positional information to the input embeddings.\n2. **Multi-Head Self-Attention**: This sub-layer performs multi-head self-attention to capture dependencies between the input embeddings.\n3. **Multi-Head Attention**: This sub-layer performs multi-head attention to capture dependencies between the output of the encoder and the decoder.\n\nThe output of each sub-layer is passed through a linear transformation and a ReLU activation function.\n\n## Attention\n\nAn attention function is as described in mapping a query and set of of-put-clause pairs to an output vector. The output of the attention function is used as the query vector.\n"
        },
        {
            "index": 3,
            "markdown": "# SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n\n## SuSeD Doc Product Attention\n## SuSeD Doc Product Attention\n## SuSeD Doc Product Attention\n## SuSeD Doc Product Attention\n## SuSeD Doc Product Attention\n## SuSeD Doc Product Attention\n##\ufffd"
        },
        {
            "index": 4,
            "markdown": "# Multi-Head Attention\n\n## Multi-Head Attention\nMulti-head attention allows the model to jointly attend to information from different representation subspaces at different positions, allowing it to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information from different representation subspaces at different positions, and to jointly attend to information at different positions, and jointly to information at different positions, and to jointly attend to information from different positions, and and and to jointly the different positions, and to jointly attend to different positions, and to different spaces, and to different spaces, anding to information at different spaces, and to jointly attend to information at different spaces, and to different spaces, and to jointly to the to understandings to jointly to different spaces, and to the time to different spaces, and to jointlying to understand the information, and to jointly to the information, and to jointlying the information, and to understand the- the different to different to different to the information, and to the to the to understand, and and and to understand the to understand the information, and to understand to understand to understandable in the understandings to understand in the to understand, and the the information, and to understand, and to understand, and to understand to understandings, and to understandings, and the the to understand to understand to understand the to the to the to the to understand to the to understand, and to understand the to understand to understand, and to understanding to understanding to understandings, and to the to understand, and the to the to the to the to understand to the and to the to the and the the the the and and and the and and and and and to understand to the tos to the to the to the to the the the the to the to understand the the to the the the to the the the the to the to the to the to the the to the to understand the to the to the to the the the to the to the the the to the the the the the to the to understanding the the the to the the the the the to the to the to understand the, to the to understand to understand the the the to the to understand to understand to the the the the the to understand the theinginging to the to understanding the the the the\ufffde"
        },
        {
            "index": 5,
            "markdown": "# Table 1: Maximum path length, per-layer complexity, and minimum number of sequential operations\n\n## Self-Attention\n\n| Layer Type | Complexity per Layer | Sequential | Maximum Path Length |\n|------------|-------------------------|-------------|------------------------|\n| Self-Attention | O(n^2)                | O(n)         | O(n)                  |\n| Self-Attention (masked) | O(n^2) + O(n)         | O(n)         | O(n)                  |\n\n## Per-Pixel Encoding\n\n| Layer Type | Complexity per Layer | Sequential | Maximum Path Length |\n|------------|-------------------------|-------------|------------------------|\n| Per-Pixel Encoding | O(n^2)                | O(n)         | O(n)                  |\n\n## Why Self-Attention?\n\nSelf-attention layers are widely used in the recurrent and convolutional neural layers. They allow us to map our variable length sequences of crafted representations into a fixed-length vector.\n\nSelf-attention layers are particularly useful for mapping our variable length sequences of crafted representations into a fixed-length vector. This is because the self-attention mechanism allows the model to learn the dependencies between the input elements.\n\nThe self-attention mechanism is also able to learn dependencies in the length of the input forward and backward signals. This is because the self-attention mechanism is able to learn the dependencies in the length of the input forward and backward signals.\n\nThe maximum path length between any pair input signal position in networks composed of self-attention layers is O(n). This is because the maximum path length between any pair input signal position in networks composed of self-attention layers is O(n).\n"
        },
        {
            "index": 6,
            "markdown": "# Natural Language and Vision Assistant\n\n## Introduction\n\nThis document provides an overview of the natural language and vision assistant developed by the assistant. The assistant is designed to understand visual content and assist users with a variety of tasks using natural language.\n\n## Training\n\n### 1. Training Data\n\nWe trained our models on the standard WMT 2014 English-German dataset, consisting of about 4.5 million sentences. We also used the English-French dataset, which contains about 3500k sentences.\n\n### 2. Hyperparameter Tuning\n\nWe used the Adam optimizer with a learning rate of 0.001, a batch size of 16, and a learning rate scheduler with a learning rate of 0.001, a decay of 0.99, and a warm-up of 100 epochs.\n\n## Registration\n\n### Types of Registration During Training\n\nWe used a simple linear schedule for the learning rate, with a learning rate of 0.001, a batch size of 16, and a learning rate scheduler with a learning rate of 0.001, a decay of 0.99, and a warm-up of 100 epochs.\n"
        },
        {
            "index": 7,
            "markdown": "# The Transformer achieves better than previous state-of-the-art for all models on the ImageNet dataset.\n\n## Table 2: The Transformer achieves better than previous state-of-the-art for all models on the ImageNet dataset.\n\n| Model  | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR-FR | EN-FR | EN-FR | EN-FR | EN-FR | EN-FR |EN-FR | EN-FR | EN-FR | EN-FR | EN-FR-FR-FR | EN-FR,EN-FR | EN-FR:EN-FR, EN-FR | EN-FR,EN-FR,EN-FR, EN-FR | EN-FR, EN-FR, EN-FR.EN-FR, EN-FR, EN-FR, EN-FR, EN-FR, EN-FR,EN-FR, EN-FR, EN-FR, EN-FR, EN-FR, EN-FR, EN-FR, EN, EN, EN-FR, EN-FR, EN-FR, EN-EN-EN, EN-EN-EN-EN-FR, EN-EN-EN, ENs, ENes, EN-EN, EN, EN-EN-EN-FR,EN, EN-EN-EN-en, EN-en, EN-en, EN-en, EN, EN, EN-EN, EN, ENs, EN, EN-ENs, ENs, ENsenals, ENs, EN, ENs2. ENs, ENore, ENs, ENes, ENs, EN- and ena. EN-en-en, ENs. ENss, ENs. ENs. EN, ENs. EN-en, ENs, EN-EN, ENs. ENing, ENs EN. and the and EN, EN, ENn, and, and and, EN-\ns. ENets\nEN\ufffding a.\nEN-en, and a.\n a. and ands, and a.\n-\n- the- andsoreding to understand to helping.\n of the languages the\nThe- ands,\ninging, ands a- and and and\n1- and\n\nsinginginging and andseringings\n\n\n\n1ssss\ns1\ufffd\ufffdsorss"
        },
        {
            "index": 8,
            "markdown": "# Table 3: Variations on the Transformer architecture\n\n## Unlabeled values are identified in blue.\n\n## Table 3.1: Variations on the Transformer architecture\n\n| Base | # tokens | # layers | # heads | # attention | # attention | # max. per. head | # train. per. word | # test. per. word | # params |\n|-----|----------|----------|---------|--------------|--------------|-------------------|-------------------|-------------------|----------|\n| (A) | 1027      | 12        | 12       | 12            | 12            | 12                | 12                 | 12                 | 12,000  |\n| (B) | 1027      | 12        | 12       | 12            | 12            | 12                | 12                 | 12                 | 12,000  |\n| (C) | 25M      | 12        | 12       | 12            | 12            | 12                | 12                 | 12                 | 12,000  |\n| (D) | 25M      | 12        | 12       | 12            | 12            | 12                | 12                 | 12                 | 12,000  |\n\n## Table 3.2: Variations on the Transformer architecture\n\n| Base | # tokens | # layers | # heads | # attention | # attention | # max. per. head | # train. per. word | # test. per. word | # params |\n|-----|----------|----------|---------|--------------|--------------|-------------------|-------------------|-------------------|----------|\n| (A) | 1027      | 12        | 12       | 12            | 12            | 12                | 12                 | 12                 | 12,000  |\n| (B) | 1027      | 12        | 12       | 12            | 12            | 12                | 12                 | 12                 | 12,000  |\n| (C) | 25M      | 12        | 12       | 12            | 12            | 12                | 12                 | 12                 | 12,000  |\n| (D) | 25M      | 12        | 12       | 12            | 12            | 12                | 12                 | 12                 | 12,000  |\n\n## Table 3.3: Variations on the Transformer architecture\n\n| Base | # tokens | # layers | # heads | # attention | # attention | # max. per. head | # train. per. word | # test. per. word | # params |\n|-----|----------|----------|---------|--------------|--------------|-------------------|-------------------|-------------------|----------|\n| (A) | 1027      | 12        | 12       | 12            | 12            | 12                | 12                 | 12                 | 12,000  |\n| (B) | 1027      | 12        | 12       | 12            | 12            | 12                | 12                 | 12                 | 12,000  |\n| (C) | 25M      | 12        | 12       | 12            | 12            | 12                | 12                 | 12                 | 12,000  |\n| (D) | 25M      | 12        | 12       | 12            | 12            | 12                | 12                 | 12                 | 12,000  |\n\n## Table 3.4: Variations on the Transformer architecture\n\n| Base | # tokens | # layers | # heads | # attention | # attention | # max. per. head | # train. per. word | # test. per. word | # params |\n|-----|----------|----------|---------|--------------|--------------|-------------------|-------------------|-------------------|----------|\n| (A) | 1027      | 12        | 12       | 12            | 12            | 12                | 12                 | 12                 | 12,000  |\n| (B) | 1027      | 12        | 12       | 12            | 12            | 12                | 12                 | 12                 | 12,000  |\n| (C) | 25M      | 12        | 12       | 12            | 12            | 12                | 12                 | 12                 | 12,000  |\n| (D) | 25M      | 12        | 12       | 12            | 12            | 12                | 12                 | 12                 | 12,000  |\n\n## English Continuation Parsing\n\n### English Continuation Parsing\n\nWe have used a large transformer with \\( L = 12 \\) and \\( d = 100 \\) as proposed in the original Transformer paper. We have also used a large transformer with \\( L = 12 \\) and \\( d = 100 \\) as proposed in the original Transformer paper. We have used a vocabulary of 100 tokens for the W2V only setting and a vocabulary of 100 tokens for the W2V only setting. We have used a vocabulary of 100 tokens for the W2V only setting and a vocabulary of 100 tokens for the W2V only setting. We have used a vocabulary of 100 tokens for the W2V only setting and a vocabulary of 100 tokens for the W2V only setting. We have used a vocabulary of 100 tokens for the W2V only setting and a vocabulary of 100 tokens for the W2V only setting. We have used a vocabulary of 100 tokens for the W2V only setting and a vocabulary of 100 tokens for the W2V only setting. We have used a vocabulary of 100 tokens for the W2V only setting and a vocabulary of 100 tokens for the W2V only setting. We have used a vocabulary of 100 tokens for the W2V only setting and a vocabulary of 100 tokens for the W2V only setting. We have used a vocabulary of 100 tokens for the W2V only setting and a vocabulary of 100 tokens for the W2V only setting. We have used a vocabulary of 100 tokens for the W2V only setting and a vocabulary of 100 tokens for the W2V only setting. We have used a vocabulary of 100 tokens for the W2V only setting and a vocabulary of 100 tokens for the W2V only setting. We have used a vocabulary of 100 tokens for the W2V only setting and a vocabulary of 100 tokens for the W2V only setting. We have used a vocabulary of 100 tokens for the W2V only setting and a vocabulary of 100 tokens for the W2V only setting. We have used a vocabulary of 100 tokens for the W2V only setting and a vocabulary of 100 tokens for the W2V only setting. We have used a vocabulary of 100 tokens for the W2V only setting and a vocabulary of 100 tokens for the W2V only setting. We have used a vocabulary of 100 tokens for the W2V only setting and a vocabulary of 100 tokens for the W2V only setting. We have used a vocabulary of 100 tokens for the W2V only setting and a vocabulary of 100 tokens for the W2V only setting. We have used a vocabulary of 100 tokens for the W2V only setting and a vocabulary of 100 tokens for the W2V only setting. We have used a vocabulary of 100 tokens for the W2V only setting and a vocabulary of 100 tokens for the W2V only setting. We have used a vocabulary of 100 tokens for the W2V only setting and a vocabulary of 100 tokens for the W2V only setting. We have used a vocabulary of 100 tokens for the W2V only setting and a vocabulary of 100 tokens for the W2V only setting. We have used a vocabulary of 100 tokens for the W2V only setting and a vocabulary of 100 tokens for the W2V only setting. We have used a vocabulary of 100 tokens for the W2V only setting and a vocabulary of 100 tokens for the W2V only setting. We have used a vocabulary of 100 tokens for the W2V only setting and a vocabulary of 100 tokens for the W2V only setting. We have used a vocabulary of 100 tokens for the W2V only setting and a vocabulary of 100 tokens for the W2V only setting. We have used a vocabulary of 100 tokens for the W2V only setting and a vocabulary of 100 tokens for the W2V only setting. We have used a vocabulary of 100 tokens for the W2V only setting and a vocabulary of 100 tokens for the W2V only setting. We have used a vocabulary of 100 tokens for the W2V only setting and a vocabulary of 100 tokens for the W2V only setting. We have used a vocabulary of 100 tokens for the W2V only setting and a vocabulary of 100 tokens for the W2V only setting. We have used a vocabulary of 100 tokens for the W2V only setting and a vocabulary of 100 tokens for the W2V only setting. We have used a vocabulary of 100 tokens for the W2V only setting and a vocabulary of 100 tokens for the W2V only setting. We have used a vocabulary of 100 tokens for the W2V only setting and a vocabulary of 100 tokens for the W2V only setting. We have used a vocabulary of 100 tokens for the W2V only setting and a vocabulary of 100 tokens for the W2V only setting. We have used a vocabulary of 100 tokens for the W2V only setting and a vocabulary of 100 tokens for the W2V only setting. We have used a vocabulary of 100 tokens for the W2V only setting and a vocabulary of 100 tokens for the W2V only setting. We have used a vocabulary of 100 tokens for the W2V only setting and a vocabulary of 100 tokens for the W2V only setting. We have used a vocabulary of 100 tokens for the W2V only setting and a vocabulary of 100 tokens for the W2V only setting. We have used a vocabulary of 100 tokens for the W2V only setting and a vocabulary of 100 tokens for the W2V only setting. We have used a vocabulary of 100 tokens for the W2V only setting and a vocabulary of 100 tokens for the W2V only setting. We have used a vocabulary of 100 tokens for the W2V only setting and a vocabulary of 100 tokens for the W2V only setting. We have used a vocabulary of 100 tokens for the W2V only setting and a vocabulary of 100 tokens for the W2V only setting. We have used a vocabulary of 100 tokens for the W2V only setting and a vocabulary of 100 tokens for the W2V only setting. We have used a vocabulary of 100 tokens for the W2V only setting and a vocabulary of 100 tokens for the W2V only setting. We have used a vocabulary of 100 tokens for the W2V only setting and a vocabulary of 100 tokens for the W2V only setting. We have used a vocabulary of 100 tokens for the W2V only setting and a vocabulary of 100 tokens for the W2V only setting. We have used a vocabulary of 100 tokens for the W2V only setting and a vocabulary of 100 tokens for the W2V only setting. We have used a vocabulary of 100 tokens for the W2V only setting and a vocabulary of 100 tokens for the W2V only setting. We have used a vocabulary of 100 tokens for the W2V only setting and a vocabulary of 100 tokens for the W2V only setting. We have used a vocabulary of 100 tokens for the W2V only setting and a vocabulary of 100 tokens for the W2V only setting. We have used a vocabulary of 100 tokens for the W2V only setting and a vocabulary of 100 tokens for the W2V only setting. We have used a vocabulary of 100 tokens for the W2V only setting and a vocabulary of 100 tokens for the W2V only setting. We have used a vocabulary of 100 tokens for the W2V only setting and a vocabulary of 100 tokens for the W2V only setting. We have used a vocabulary of 100 tokens for the W2V only setting and a vocabulary of 100 tokens for the W2V only setting. We have used a vocabulary of 100 tokens for the W2V only setting and a vocabulary of 100 tokens for the W2V only setting. We have used a vocabulary of 100 tokens for the W2V only setting and a vocabulary of 100 tokens for the W2V only setting. We have used a vocabulary of 100 tokens for the W2V only setting and a vocabulary of 100 tokens for the W2V only setting. We have used a vocabulary of 100 tokens for the W2V only setting and a vocabulary of 100 tokens for the W2V only setting. We have used a vocabulary of 100 tokens for the W2V only setting and a vocabulary of 100 tokens for the W2V only setting. We have used a vocabulary of 100 tokens for the W2V only setting and a vocabulary of 100 tokens for the W2V only setting. We have used a vocabulary of 100 tokens for the W2V only setting and a vocabulary of 100 tokens for the W2V only setting. We have used a vocabulary of 100 tokens for the W2V only setting and a vocabulary of 100 tokens for the W2V only setting. We have used a vocabulary of 100 tokens for the W2V only setting and a vocabulary of 100 tokens for the W2V only setting. We have used a vocabulary of 100 tokens for the W2V only setting and a vocabulary of 100 tokens for the W2V only setting. We have used a vocabulary of 100 tokens for the W2V only setting and a vocabulary of 100 tokens for the W2V only setting. We have used a vocabulary of 100 tokens for the W2V only setting and a vocabulary of 100 tokens for the W2V only setting and a vocabulary of 100 tokens for the W2V only setting and a vocabulary of 100 tokens for the W2V only setting and a vocabulary of 100."
        },
        {
            "index": 9,
            "markdown": "# Table of The Transformer generators with its English concordance parsing Results on Section 2.1\n\n## Table 1: WMT 2018 English-German results\n\n| Model | WMT 2018 | WMT 2017 | WMT 2016 |\n|------|--------|--------|--------|\n| WMT 2018 | 96.0 | 96.0 | 96.0 |\n| WMT 2017 | 96.0 | 96.0 | 96.0 |\n| WMT 2016 | 96.0 | 96.0 | 96.0 |\n\n## Table 2: WMT 2018 English-French results\n\n| Model | WMT 2018 | WMT 2017 | WMT 2016 |\n|------|--------|--------|--------|\n| WMT 2018 | 96.0 | 96.0 | 96.0 |\n| WMT 2017 | 96.0 | 96.0 | 96.0 |\n| WMT 2016 | 96.0 | 96.0 | 96.0 |\n\n## Table 3: WMT 2018 English-Italian results\n\n| Model | WMT 2018 | WMT 2017 | WMT 2016 |\n|------|--------|--------|--------|\n| WMT 2018 | 96.0 | 96.0 | 96.0 |\n| WMT 2017 | 96.0 | 96.0 | 96.0 |\n| WMT 2016 | 96.0 | 96.0 | 96.0 |\n\n## Table 4: WMT 2018 English-Portuguese results\n\n| Model | WMT 2018 | WMT 2017 | WMT 2016 |\n|------|--------|--------|--------|\n| WMT 2018 | 96.0 | 96.0 | 96.0 |\n| WMT 2017 | 96.0 | 96.0 | 96.0 |\n| WMT 2016 | 96.0 | 96.0 | 96.0 |\n\n## Conclusion\n\nThe Transformer model based entirely on attention, replacing the recurrent layers with convolutional layers, and encoder-decoder architecture with self-attention mechanism, has achieved state-of-the-art results on WMT 2014 English-German, WMT 2014 English-French, and WMT 2018 English-French translation tasks. We are excited to see the Transformer model can be applied to other tasks. We are excited to investigate the extended attention mechanism to efficiently handle large-scale and sequential data.\n\nAcknowledgements:\nWe are grateful to Nat Friedman and Stephen Gasson for their helpful comments.\n\nReferences:\n\n(1) Wang, Jian, Ryan Kiros, and Geoffrey E. Hinton. \"Neural machine translation by jointly learning to align and translate.\" arXiv preprint arXiv:1402.1590, 2014.\n\n(2) Kornbluth, Chris, et al. \"Recurrent neural networks for machine translation.\" arXiv preprint arXiv:1402.1590, 2014.\n\n(3) Wang, Jian, et al. \"Long short-term memory networks for machine translation.\" arXiv preprint arXiv:1402.1590, 2014.\n\n(4) Yang, Jian, et al. \"Long short-term memory networks for machine translation.\" arXiv preprint arXiv:1402.1590, 2014.\n"
        },
        {
            "index": 10,
            "markdown": "# Page Content\n\n## Headers\n\n- **Title**: Page Content\n- **Subtitle**: A Markdown representation of the page\n\n## Content\n\n1. **Header 1**\n   - **Subheader**: Subheader content\n\n2. **Header 2**\n   - **Subheader**: Subheader content\n\n3. **Header 3**\n   - **Subheader**: Subheader content\n\n4. **Header 4**\n   - **Subheader**: Subheader content\n\n5. **Header 5**\n   - **Subheader**: Subheader content\n\n6. **Header 6**\n   - **Subheader**: Subheader content\n\n7. **Header 7**\n   - **Subheader**: Subheader content\n\n8. **Header 8**\n   - **Subheader**: Subheader content\n\n9. **Header 9**\n   - **Subheader**: Subheader content\n\n10. **Header 10**\n    - **Subheader**: Subheader content\n\n11. **Header 11**\n    - **Subheader**: Subheader content\n\n12. **Header 12**\n    - **Subheader**: Subheader content\n\n13. **Header 13**\n    - **Subheader**: Subheader content\n\n14. **Header 14**\n    - **Subheader**: Subheader content\n\n15. **Header 15**\n    - **Subheader**: Subheader content\n\n16. **Header 16**\n    - **Subheader**: Subheader content\n\n17. **Header 17**\n    - **Subheader**: Subheader content\n\n18. **Header 18**\n    - **Subheader**: Subheader content\n\n19. **Header 19**\n    - **Subheader**: Subheader content\n\n20. **Header 20**\n    - **Subheader**: Subheader content\n\n21. **Header 21**\n    - **Subheader**: Subheader content\n\n22. **Header 22**\n    - **Subheader**: Subheader content\n\n23. **Header 23**\n    - **Subheader**: Subheader content\n\n24. **Header 24**\n    - **Subheader**: Subheader content\n\n25. **Header 25**\n    - **Subheader**: Subheader content\n\n26. **Header 26**\n    - **Subheader**: Subheader content\n\n27. **Header 27**\n    - **Subheader**: Subheader content\n\n28. **Header 28**\n    - **Subheader**: Subheader content\n\n29. **Header 29**\n    - **Subheader**: Subheader content\n\n30. **Header 30**\n    - **Subheader**: Subheader content\n\n31. **Header 31**\n    - **Subheader**: Subheader content\n\n32. **Header 32**\n    - **Subheader**: Subheader content\n\n33. **Header 33**\n    - **Subheader**: Subheader content\n\n34. **Header 34**\n    - **Subheader**: Subheader content\n\n35. **Header 35**\n    - **Subheader**: Subheader content\n\n36. **Header 36**\n    - **Subheader**: Subheader content\n\n37. **Header 37**\n    - **Subheader**: Subheader content\n\n38. **Header 38**\n    - **Subheader**: Subheader content\n\n39. **Header 39**\n    - **Subheader**: Subheader content\n\n40. **Header 40**\n    - **Subheader**: Subheader content\n\n41. **Header 41**\n    - **Subheader**: Subheader content\n\n42. **Header 42**\n    - **Subheader**: Subheader content\n\n43. **Header 43**\n    - **Subheader**: Subheader content\n\n44. **Header 44**\n    - **Subheader**: Subheader content\n\n45. **Header 45**\n    - **Subheader**: Subheader content\n\n46. **Header 46**\n    - **Subheader**: Subheader content\n\n47. **Header 47**\n    - **Subheader**: Subheader content\n\n48. **Header 48**\n    - **Subheader**: Subheader content\n\n49. **Header 49**\n    - **Subheader**: Subheader content\n\n50. **Header 50**\n    - **Subheader**: Subheader content\n\n51. **Header 51**\n    - **Subheader**: Subheader content\n\n52. **Header 52**\n    - **Subheader**: Subheader content\n\n53. **Header 53**\n    - **Subheader**: Subheader content\n\n54. **Header 54**\n    - **Subheader**: Subheader content\n\n55. **Header 55**\n    - **Subheader**: Subheader content\n\n56. **Header 56**\n    - **Subheader**: Subheader content\n\n57. **Header 57**\n    - **Subheader**: Subheader content\n\n58. **Header 58**\n    - **Subheader**: Subheader content\n\n59. **Header 59**\n    - **Subheader**: Subheader content\n\n60. **Header 60**\n    - **Subheader**: Subheader content\n\n61. **Header 61**\n    - **Subheader**: Subheader content\n\n62. **Header 62**\n    - **Subheader**: Subheader content\n\n63. **Header 63**\n    - **Subheader**: Subheader content\n\n64. **Header 64**\n    - **Subheader**: Subheader content\n\n65. **Header 65**\n    - **Subheader**: Subheader content\n\n66. **Header 66**\n    - **Subheader**: Subheader content\n\n67. **Header 67**\n    - **Subheader**: Subheader content\n\n68. **Header 68**\n    - **Subheader**: Subheader content\n\n69. **Header 69**\n    - **Subheader**: Subheader content\n\n70. **Header 70**\n    - **Subheader**: Subheader content\n\n71. **Header 71**\n    - **Subheader**: Subheader content\n\n72. **Header 72**\n    - **Subheader**: Subheader content\n\n73. **Header 73**\n    - **Subheader**: Subheader content\n\n74. **Header 74**\n    - **Subheader**: Subheader content\n\n75. **Header 75**\n    - **Subheader**: Subheader content\n\n76. **Header 76**\n    - **Subheader**: Subheader content\n\n77. **Header 77**\n    - **Subheader**: Subheader content\n\n78. **Header 78**\n    - **Subheader**: Subheader content\n\n79. **Header 79**\n    - **Subheader**: Subheader content\n\n80. **Header 80**\n    - **Subheader**: Subheader content\n\n81. **Header 81**\n    - **Subheader**: Subheader content\n\n82. **Header 82**\n    - **Subheader**: Subheader content\n\n83. **Header 83**\n    - **Subheader**: Subheader content\n\n84. **Header 84**\n    - **Subheader**: Subheader content\n\n85. **Header 85**\n    - **Subheader**: Subheader content\n\n86. **Header 86**\n    - **Subheader**: Subheader content\n\n87. **Header 87**\n    - **Subheader**: Subheader content\n\n88. **Header 88**\n    - **Subheader**: Subheader content\n\n89. **Header 89**\n    - **Subheader**: Subheader content\n\n90. **Header 90**\n    - **Subheader**: Subheader content\n\n91. **Header 91**\n    - **Subheader**: Subheader content\n\n92. **Header 92**\n    - **Subheader**: Subheader content\n\n93. **Header 93**\n    - **Subheader**: Subheader content\n\n94. **Header 94**\n    - **Subheader**: Subheader content\n\n95. **Header 95**\n    - **Subheader**: Subheader content\n\n96. **Header 96**\n    - **Subheader**: Subheader content\n\n97. **Header 97**\n    - **Subheader**: Subheader content\n\n98. **Header 98**\n    - **Subheader**: Subheader content\n\n99. **Header 99**\n    - **Subheader**: Subheader content\n\n100. **Header 100**\n    - **Subheader**: Subheader content\n\n101. **Header 101**\n    - **Subheader**: Subheader content\n\n102. **Header 102**\n    - **Subheader**: Subheader content\n\n103. **Header 103**\n    - **Subheader**: Subheader content\n\n104. **Header 104**\n    - **Subheader**: Subheader content\n\n105. **Header 105**\n    - **Subheader**: Subheader content\n\n106. **Header 106**\n    - **Subheader**: Subheader content\n\n107. **Header 107**\n    - **Subheader**: Subheader content\n\n108. **Header 108**\n    - **Subheader**: Subheader content\n\n109. **Header 109**\n    - **Subheader**: Subheader content\n\n110. **Header 110**\n    - **Subheader**: Subheader content\n\n111. **Header 111**\n    - **Subheader**: Subheader content\n\n112. **Header 112**\n    - **Subheader**: Subheader content\n\n113. **Header 113**\n    - **Subheader**: Subheader content\n\n114. **Header 114**\n    - **Subheader**: Subheader content\n\n115. **Header 115**\n    - **Subheader**: Subheader content\n\n116. **Header 116**\n    - **Subheader**: Subheader content\n\n117. **Header 117**\n    - **Subheader**: Subheader content\n\n118. **Header 118**\n    - **Subheader**: Subheader content\n\n119. **Header 119**\n    - **Subheader**: Subheader content\n\n120. **Header 120**\n    - **Subheader**: Subheader content\n\n121. **Header 121**\n    - **Subheader**: Subheader content\n\n122. **Header 122**\n    - **Subheader**: Subheader content\n\n123. **Header 123**\n    - **Subheader**: Subheader content\n\n124. **Header 124**\n    - **Subheader**: Subheader content\n\n125. **Header 125**\n    - **Subheader**: Subheader content\n\n126. **Header 126**\n    - **Subheader**: Subheader content\n\n127. **Header 127**\n    - **Subheader**: Subheader content\n\n128. **Header 128**\n    - **Subheader**: Subheader content\n\n129. **Header 129**\n    - **Subheader**: Subheader content\n\n130. **Header 130**\n    - **Subheader**: Subheader content\n\n131. **Header 131**\n    - **Subheader**: Subheader content\n\n132. **Header 132**\n    - **Subheader**: Subheader content\n\n133. **Header 133**\n    - **Subheader**: Subheader content\n\n134. **Header 134**\n    - **Subheader**: Subheader content\n\n135. **Header 135**\n    - **Subheader**: Subheader content\n\n136. **Header 136**\n    - **Subheader**: Subheader content\n\n137. **Header 137**\n    - **Subheader**: Subheader content\n\n138. **Header 138**\n    - **Subheader**: Subheader content\n\n139. **Header 139**\n    - **Subheader**: Subheader content\n\n140. **Header 140**\n    - **Subheader**: Subheader content\n\n141. **Header 141**\n    - **Subheader**: Subheader content\n\n142. **Header 142**\n    - **Subheader**: Subheader content\n\n143. **Header 143**\n    - **Subheader**: Subheader content\n\n144. **Header 144**\n    - **Subheader**: Subheader content\n\n145. **Header 145**\n    - **Subheader**: Subheader content\n\n146. **Header 146**\n    - **Subheader**: Subheader content\n\n147. **Header 147**\n    - **Subheader**: Subheader content\n\n148. **Header 148**\n    - **Subheader**: Subheader content\n\n149. **Header 149**\n    - **Subheader**: Subheader content\n\n150. **Header 150**\n    - **Subheader**: Subheader content\n\n151. **Header 151**\n    - **Subheader**: Subheader content\n\n152. **Header 152**\n    - **Subheader**: Subheader content\n\n153. **Header 153**\n    - **Subheader**: Subheader content\n\n154. **Header 154**\n    - **Subheader**: Subheader content\n\n155. **Header 155**\n    - **Subheader**: Subheader content\n\n156. **Header 156**\n    - **Subheader**: Subheader content\n\n157. **Header 157**\n    - **Subheader**: Subheader content\n\n158. **Header 159**\n    - **Subheader**: Subheader content\n\n160. **Header 160**\n    - **Subheader**: Subheader content\n\n161. **Header 161**\n    - **Subheader**: Subheader content\n\n162. **Header 162. **Subheader**: Subheader content\n\n163. **Subheader**: Subheader content\n\n164. **Subheader**: Subheader: Subheader content\n\n165. **Subheader: Subheader\n\n166. **Subheader: Subheader: Subheader: Subheader: Subheader: Subheader: Subheader: Subheader: Subheader: Subheader: Subheader: Subheader: Subheader: Subheader: Subheader: Subheader: Subheader: Subheader: Subheader: Subheader: Subheader: Subheader: Subheader: Subheader: Subheader: Subheader: Subheader: Subheader: Subheader: Subheader: Subheader: Subheader: Subheader: Subheader: Subheader: Subheader: Subheader: Subheader: Subheader: Subheader: Subheader: Subheader: Subheader: Subheader: Subheader: Subheader: Subheader: Subheader: Subheader: Subheader: Subheader: Subheader: Subheader: Sub, Sub, and the- Subheader: Subheader: Sub."
        },
        {
            "index": 11,
            "markdown": "# Page 12\n\n## Table of Contents\n\n1. [Maike M\ufffdller, Mary Jan MacIntosh, and R\u00e9nier S\u00e9n\u00e9c\u00e9, Building large-personalized models for visual scene understanding](https://www.researchgate.net/publication/334265533_Building_large-personalized_models_for_visual_scene_understanding)\n2. [David McQuaide, Eugene Choula, and Mark Johnson, Effective self-training for growing, in-the-wild, and online models](https://arxiv.org/abs/1903.08554)\n3. [Peter Park, One-to-Many, Disparities, and Multi-Utterances: A downscalable attention model for Disparities in Natural Language Processing](https://arxiv.org/abs/1903.08554)\n4. [Rui Pinto, Low-Wall, Using the output embedding to improve language models, arXiv preprint, 2019](https://arxiv.org/abs/1903.08554)\n5. [Rui Pinto, Low-Wall, Using the output embedding to improve language models, arXiv preprint, 2019](https://arxiv.org/abs/1903.08554)\n6. [Rui Pinto, Low-Wall, Using the output embedding to improve language models, arXiv preprint, 2019](https://arxiv.org/abs/1903.08554)\n7. [Rui Pinto, Low-Wall, Using the output embedding to improve language models, arXiv preprint, 2019](https://arxiv.org/abs/1903.08554)\n8. [Rui Pinto, Low-Wall, Using the output embedding to improve language models, arXiv preprint, 2019](https://arxiv.org/abs/1903.08554)\n9. [Rui Pinto, Low-Wall, Using the output embedding to improve language models, arXiv preprint, 2019](https://arxiv.org/abs/1903.08554)\n10. [Rui Pinto, Low-Wall, Using the output embedding to improve language models, arXiv preprint, 2019](https://arxiv.org/abs/1903.08554)\n11. [Rui Pinto, Low-Wall, Using the output embedding to improve language models, arXiv preprint, 2019](https://arxiv.org/abs/1903.08554)\n12. [Rui Pinto, Low-Wall, Using the output embedding to improve language models, arXiv preprint, 2019](https://arxiv.org/abs/1903.08554)\n13. [Rui Pinto, Low-Wall, Using the output embedding to improve language models, arXiv preprint, 2019](https://arxiv.org/abs/1903.08554)\n14. [Rui Pinto, Low-Wall, Using the output embedding to improve language models, arXiv preprint, 2019](https://arxiv.org/abs/1903.08554)\n15. [Rui Pinto, Low-Wall, Using the output embedding to improve language models, arXiv preprint, 2019](https://arxiv.org/abs/1903.08554)\n16. [Rui Pinto, Low-Wall, Using the output embedding to improve language models, arXiv preprint, 2019](https://arxiv.org/abs/1903.08554)\n17. [Vaughn M. Kavan, Ron, and Sudarshan, G., Grammarian as a foreign language: In-the-wild, in-the-wild, and in-the-wild](https://arxiv.org/abs/1903.08554)\n18. [Vaughn M. Kavan, Ron, and Sudarshan, G., Grammarian as a foreign language: In-the-wild, in-the-wild, and in-the-wild](https://arxiv.org/abs/1903.08554)\n19. [Vaughn M. Kavan, Ron, and Sudarshan, G., Grammarian as a foreign language: In-the-wild, in-the-wild, and in-the-wild](https://arxiv.org/abs/1903.08554)\n20. [Vaughn M. Kavan, Ron, and Sudarshan, G., Grammarian as a foreign language: In-the-wild, in-the-wild, and in-the-wild](https://arxiv.org/abs/1903.08554)\n21. [Vaughn M. Kavan, Ron, and Sudarshan, G., Grammarian as a foreign language: In-the-wild, in-the-wild, and in-the-wild](https://arxiv.org/abs/1903.08554)\n22. [Vaughn M. Kavan, Ron, and Sudarshan, G., Grammarian as a foreign language: In-the-wild, in-the-wild, and in-the-wild](https://arxiv.org/abs/1903.08554)\n23. [Vaughn M. Kavan, Ron, and Sudarshan, G., Grammarian as a foreign language: In-the-wild, in-the-wild, and in-the-wild](https://arxiv.org/abs/1903.08554)\n24. [Vaughn M. Kavan, Ron, and Sudarshan, G., Grammarian as a foreign language: In-the-wild, in-the-wild, and in-the-wild](https://arxiv.org/abs/1903.08554)\n25. [Vaughn M. Kavan, Ron, and Sudarshan, G., Grammarian as a foreign language: In-the-wild, in-the-wild, and in-the-wild](https://arxiv.org/abs/1903.08554)\n26. [Vaughn M. Kavan, Ron, and Sudarshan, G., Grammarian as a foreign language: In-the-wild, in-the-wild, and in-the-wild](https://arxiv.org/abs/1903.08554)\n27. [Vaughn M. Kavan, Ron, and Sudarshan, G., Grammarian as a foreign language: In-the-wild, in-the-wild, and in-the-wild](https://arxiv.org/abs/1903.08554)\n28. [Vaughn M. Kavan, Ron, and Sudarshan, G., Grammarian as a foreign language: In-the-wild, in-the-wild, and in-the-wild](https://arxiv.org/abs/1903.08554)\n29. [Vaughn M. Kavan, Ron, and Sudarshan, G., Grammarian as a foreign language: In-the-wild, in-the-wild, and in-the-wild](https://arxiv.org/abs/1903.08554)\n30. [Vaughn M. Kavan, Ron, and Sudarshan, G., Grammarian as a foreign language: In-the-wild, in-the-wild, and in-the-wild](https://arxiv.org/abs/1903.08554)\n31. [Vaughn M. Kavan, Ron, and Sudarshan, G., Grammarian as a foreign language: In-the-wild, in-the-wild, and in-the-wild](https://arxiv.org/abs/1903.08554)\n32. [Vaughn M. Kavan, Ron, and Sudarshan, G., Grammarian as a foreign language: In-the-wild, in-the-wild, and in-the-wild](https://arxiv.org/abs/1903.08554)\n33. [Vaughn M. Kavan, Ron, and Sudarshan, G., Grammarian as a foreign language: In-the-wild, in-the-wild, and in-the-wild](https://arxiv.org/abs/1903.08554)\n34. [Vaughn M. Kavan, Ron, and Sudarshan, G., Grammarian as a foreign language: In-the-wild, in-the-wild, and in-the-wild](https://arxiv.org/abs/1903.08554)\n35. [Vaughn M. Kavan, Ron, and Sudarshan, G., Grammarian as a foreign language: In-the-wild, in-the-wild, and in-the-wild](https://arxiv.org/abs/1903.08554)\n36. [Vaughn M. Kavan, Ron, and Sudarshan, G., Grammarian as a foreign language: In-the-wild, in-the-wild, and in-the-wild](https://arxiv.org/abs/1903.08554)\n37. [Vaughn M. Kavan, Ron, and Sudarshan, G., Grammarian as a foreign language: In-the-wild, in-the-wild, and in-the-wild](https://arxiv.org/abs/1903.08554)\n38. [Vaughn M. Kavan, Ron, and Sudarshan, G., Grammarian as a foreign language: In-the-wild, in-the-wild, and in-the-wild](https://arxiv.org/abs/1903.08554)\n39. [Vaughn M. Kavan, Ron, and Sudarshan, G., Grammarian as a foreign language: In-the-wild, in-the-wild, and in-the-wild](https://arxiv.org/abs/1903.08554)\n40. [Vaughn M. Kavan, Ron, and Sudarshan, G., Grammarian as a foreign language: In-the-wild, in-the-wild, and in-the-wild](https://arxiv.org/abs/1903.08554)\n41. [Vaughn M. Kavan, Ron, and Sudarshan, G., Grammarian as a foreign language: In-the-wild, in-the-wild, and in-the-wild](https://arxiv.org/abs/1903.08554)\n42. [Vaughn M. Kavan, Ron, and Sudarshan, G., Grammarian as a foreign language: In-the-wild, in-the-wild, and in-the-wild](https://arxiv.org/abs/1903.08554)\n43. [Vaughn M. Kavan, Ron, and Sudarshan, G., Grammarian as a foreign language: In-the-wild, in-the-wild, and in-the-wild](https://arxiv.org/abs/1903.08554)\n44. [Vaughn M. Kavan, Ron, and Sudarshan, G., Grammarian as a foreign language: In-the-wild, in-the-wild, and in-the-wild](https://arxiv.org/abs/1903.08554)\n45. [Vaughn M. Kavan, Ron, and Sudarshan, G., Grammarian as a foreign language: In-the-wild, in-the-wild, and in-the-wild](https://arxiv.org/abs/1903.08554)\n46. [Vaughn M. Kavan, Ron, and Sudarshan, G., Grammarian as a foreign language: In-the-wild, in-the-wild, and in-the-wild](https://arxiv.org/abs/1903.08554)\n47. [Vaughn M. Kavan, Ron, and Sudarshan, G., Grammarian as a foreign language: In-the-wild, in-the-wild, and in-the-wild](https://arxiv.org/abs/1903.08554)\n48. [Vaughn M. Kavan, Ron, and Sudarshan, G., Grammarian as a foreign language: In-the-wild, in-the-wild, and in-the-wild](https://arxiv.org/abs/1903.08554)\n49. [Vaughn M. Kavan, Ron, and Sudarshan, G., Grammarian as a foreign language: In-the-wild, in-the-wild, and in-the-wild](https://arxiv.org/abs/1903.08554)\n50. [Vaughn M. Kavan, Ron, and Sudarshan, G., Grammarian as a foreign language: In-the-wild, in-the-wild, and in-the-wild](https://arxiv.org/abs/1903.08554)\n51. [Vaughn M. Kavan, Ron, and Sudarshan, G., Grammarian as a foreign language: In-the-wild, in-the-wild, and in-the-wild](https://arxiv.org/abs/1903.08554)\n52. [Vaughn M. Kavan, Ron, and Sudarshan, G., Grammarian as a foreign language: In-the-wild, in-the-wild, and in-the-wild](https://arxiv.org/abs/1903.08554)\n53. [Vaughn M. Kavan, Ron, and Sudarshan, G., Grammarian as a foreign language: In-the-wild, in-the-wild, and in-the-wild](https://arxiv.org/abs/1903.08554)\n54. [Vaughn M. Kavan, Ron, and Sudarshan, G., Grammarian as a foreign language: In-the-wild, in-the-wild, and in-the-wild](https://arxiv.org/abs/1903.08554)\n55. [Vaughn M. Kavan, Ron, and Sudarshan, G., Grammarian as a foreign language: In-the-wild, in-the-wild, and in-the-wild. (arXiv.org/abs/19030554)\n56. [Vaughn M. Kavan, Ron, and Sudarshan, G., Grammarian as a foreign language: In-the-wild, in-the-wild, and in-the-wild. (arXiv.org/abs/19030554. [A)"
        },
        {
            "index": 12,
            "markdown": "# Attention Visualizations\n\n## Figure 1: An example of the attention mechanism following long-distance dependencies in the 3D attention head of the transformer.\n\n## Subtext:\n\nIn Figure 1, we show an example of the attention mechanism following long-distance dependencies in the 3D attention head of the transformer. The attention weights are visualized as a heatmap, where the x-axis and y-axis represent the input tokens. The color intensity represents the attention weights, with darker colors indicating higher attention weights. The color bar on the right side of the figure shows the range of attention weights.\n\n## Subtext:\n\nThe attention mechanism is a key component of the transformer model, which has been shown to be effective in capturing long-range dependencies in the input data. In this example, we can see that the attention weights are distributed across the input tokens, indicating that the model is able to capture dependencies between tokens that are far apart in the input sequence.\n\n## Subtext:\n\nThe attention mechanism has been shown to be effective in improving the performance of transformer models in various tasks, including machine translation, text classification, and question answering. However, it can also be challenging to interpret the attention weights, as they can be very large and difficult to visualize.\n\n## Subtext:\n\nIn this example, we can see that the attention weights are distributed across the input tokens, indicating that the model is able to capture dependencies between tokens that are far apart in the input sequence. The attention weights are also distributed across the input tokens, indicating that the model is able to capture dependencies between tokens that are far apart in the input sequence.\n\n## Subtext:\n\nThe attention mechanism has been shown to be effective in improving the performance of transformer models in various tasks, including machine translation, text classification, and question answering. However, it can also be challenging to interpret the attention weights, as they can be very large and difficult to visualize.\n\n## Subtext:\n\nIn this example, we can see that the attention weights are distributed across the input tokens, indicating that the model is able to capture dependencies between tokens that are far apart in the input sequence. The attention weights are also distributed across the input tokens, indicating that the model is able to capture dependencies between tokens that are far apart in the input sequence.\n\n## Subtext:\n\nThe attention mechanism has been shown to be effective in improving the performance of transformer models in various tasks, including machine translation, text classification, and question answering. However, it can also be challenging to interpret the attention weights, as they can be very large and difficult to visualize.\n\n## Subtext:\n\nIn this example, we can see that the attention weights are distributed across the input tokens, indicating that the model is able to capture dependencies between tokens that are far apart in the input sequence. The attention weights are also distributed across the input tokens, indicating that the model is able to capture dependencies between tokens that are far apart in the input sequence.\n\n## Subtext:\n\nThe attention mechanism has been shown to be effective in improving the performance of transformer models in various tasks, including machine translation, text classification, and question answering. However, it can also be challenging to interpret the attention weights, as they can be very large and difficult to visualize.\n\n## Subtext:\n\nIn this example, we can see that the attention weights are distributed across the input tokens, indicating that the model is able to capture dependencies between tokens that are far apart in the input sequence. The attention weights are also distributed across the input tokens, indicating that the model is able to capture dependencies between tokens that are far apart in the input sequence.\n\n## Subtext:\n\nThe attention mechanism has been shown to be effective in improving the performance of transformer models in various tasks, including machine translation, text classification, and question answering. However, it can also be challenging to interpret the attention weights, as they can be very large and difficult to visualize.\n\n## Subtext:\n\nIn this example, we can see that the attention weights are distributed across the input tokens, indicating that the model is able to capture dependencies between tokens that are far apart in the input sequence. The attention weights are also distributed across the input tokens, indicating that the model is able to capture dependencies between tokens that are far apart in the input sequence.\n\n## Subtext:\n\nThe attention mechanism has been shown to be effective in improving the performance of transformer models in various tasks, including machine translation, text classification, and question answering. However, it can also be challenging to interpret the attention weights, as they can be very large and difficult to visualize.\n\n## Subtext:\n\nIn this example, we can see that the attention weights are distributed across the input tokens, indicating that the model is able to capture dependencies between tokens that are far apart in the input sequence. The attention weights are also distributed across the input tokens, indicating that the model is able to capture dependencies between tokens that are far apart in the input sequence.\n\n## Subtext:\n\nThe attention mechanism has been shown to be effective in improving the performance of transformer models in various tasks, including machine translation, text classification, and question answering. However, it can also be challenging to interpret the attention weights, as they can be very large and difficult to visualize.\n\n## Subtext:\n\nIn this example, we can see that the attention weights are distributed across the input tokens, indicating that the model is able to capture dependencies between tokens that are far apart in the input sequence. The attention weights are also distributed across the input tokens, indicating that the model is able to capture dependencies between tokens that are far apart in the input sequence.\n\n## Subtext:\n\nThe attention mechanism has been shown to be effective in improving the performance of transformer models in various tasks, including machine translation, text classification, and question answering. However, it can also be challenging to interpret the attention weights, as they can be very large and difficult to visualize.\n\n## Subtext:\n\nIn this example, we can see that the attention weights are distributed across the input tokens, indicating that the model is able to capture dependencies between tokens that are far apart in the input sequence. The attention weights are also distributed across the input tokens, indicating that the model is able to capture dependencies between tokens that are far apart in the input sequence.\n\n## Subtext:\n\nThe attention mechanism has been shown to be effective in improving the performance of transformer models in various tasks, including machine translation, text classification, and question answering. However, it can also be challenging to interpret the attention weights, as they can be very large and difficult to visualize.\n\n## Subtext:\n\nIn this example, we can see that the attention weights are distributed across the input tokens, indicating that the model is able to capture dependencies between tokens that are far apart in the input sequence. The attention weights are also distributed across the input tokens, indicating that the model is able to capture dependencies between tokens that are far apart in the input sequence.\n\n## Subtext:\n\nThe attention mechanism has been shown to be effective in improving the performance of transformer models in various tasks, including machine translation, text classification, and question answering. However, it can also be challenging to interpret the attention weights, as they can be very large and difficult to visualize.\n\n## Subtext:\n\nIn this example, we can see that the attention weights are distributed across the input tokens, indicating that the model is able to capture dependencies between tokens that are far apart in the input sequence. The attention weights are also distributed across the input tokens, indicating that the model is able to capture dependencies between tokens that are far apart in the input sequence.\n\n## Subtext:\n\nThe attention mechanism has been shown to be effective in improving the performance of transformer models in various tasks, including machine translation, text classification, and question answering. However, it can also be challenging to interpret the attention weights, as they can be very large and difficult to visualize.\n\n## Subtext:\n\nIn this example, we can see that the attention weights are distributed across the input tokens, indicating that the model is able to capture dependencies between tokens that are far apart in the input sequence. The attention weights are also distributed across the input tokens, indicating that the model is able to capture dependencies between tokens that are far apart in the input sequence.\n\n## Subtext:\n\nThe attention mechanism has been shown to be effective in improving the performance of transformer models in various tasks, including machine translation, text classification, and question answering. However, it can also be challenging to interpret the attention weights, as they can be very large and difficult to visualize.\n\n## Subtext:\n\nIn this example, we can see that the attention weights are distributed across the input tokens, indicating that the model is able to capture dependencies between tokens that are far apart in the input sequence. The attention weights are also distributed across the input tokens, indicating that the model is able to capture dependencies between tokens that are far apart in the input sequence.\n\n## Subtext:\n\nThe attention mechanism has been shown to be effective in improving the performance of transformer models in various tasks, including machine translation, text classification, and question answering. However, it can also be challenging to interpret the attention weights, as they can be very large and difficult to visualize.\n\n## Subtext:\n\nIn this example, we can see that the attention weights are distributed across the input tokens, indicating that the model is able to capture dependencies between tokens that are far apart in the input sequence. The attention weights are also distributed across the input tokens, indicating that the model is able to capture dependencies between tokens that are far apart in the input sequence.\n\n## Subtext:\n\nThe attention mechanism has been shown to be effective in improving the performance of transformer models in various tasks, including machine translation, text classification, and question answering. However, it can also be challenging to interpret the attention weights, as they can be very large and difficult to visualize.\n\n## Subtext:\n\nIn this example, we can see that the attention weights are distributed across the input tokens, indicating that the model is able to capture dependencies between tokens that are far apart in the input sequence. The attention weights are also distributed across the input tokens, indicating that the model is able to capture dependencies between tokens that are far apart in the input sequence.\n\n## Subtext:\n\nThe attention mechanism has been shown to be effective in improving the performance of transformer models in various tasks, including machine translation, text classification, and question answering. However, it can also be challenging to interpret the attention weights, as they can be very large and difficult to visualize.\n\n## Subtext:\n\nIn this example, we can see that the attention weights are distributed across the input tokens, indicating that the model is able to capture dependencies between tokens that are far apart in the input sequence. The attention weights are also distributed across the input tokens, indicating that the model is able to capture dependencies between tokens that are far apart in the input sequence.\n\n## Subtext:\n\nThe attention mechanism has been shown to be effective in improving the performance of transformer models in various tasks, including machine translation, text classification, and question answering. However, it can also be challenging to interpret the attention weights, as they can be very large and difficult to visualize.\n\n## Subtext:\n\nIn this example, we can see that the attention weights are distributed across the input tokens, indicating that the model is able to capture dependencies between tokens that are far apart in the input sequence. The attention weights are also distributed across the input tokens, indicating that the model is able to capture dependencies between tokens that are far apart in the input sequence.\n\n## Subtext:\n\nThe attention mechanism has been shown to be effective in improving the performance of transformer models in various tasks, including machine translation, text classification, and question answering. However, it can also be challenging to interpret the attention weights, as they can be very large and difficult to visualize.\n\n## Subtext:\n\nIn this example, we can see that the attention weights are distributed across the input tokens, indicating that the model is able to capture dependencies between tokens that are far apart in the input sequence. The attention weights are also distributed across the input tokens, indicating that the model is able to capture dependencies between tokens that are far apart in the input sequence.\n\n## Subtext:\n\nThe attention mechanism has been shown to be effective in improving the performance of transformer models in various tasks, including machine translation, text classification, and question answering. However, it can also be challenging to interpret the attention weights, as they can be very large and difficult to visualize.\n\n## Subtext:\n\nIn this example, we can see that the attention weights are distributed across the input tokens, indicating that the model is able to capture dependencies between tokens that are far apart in the input sequence. The attention weights are also distributed across the input tokens, indicating that the model is able to capture dependencies between tokens that are far apart in the input sequence.\n\n## Subtext:\n\nThe attention mechanism has been shown to be effective in improving the performance of transformer models in various tasks, including machine translation, text classification, and question answering. However, it can also be challenging to interpret the attention weights, as they can be very large and difficult to visualize.\n\n## Subtext:\n\nIn this example, we can see that the attention weights are distributed across the input tokens, indicating that the model is able to capture dependencies between tokens that are far apart in the input sequence. The attention weights are also distributed across the input tokens, indicating that the model is able to capture dependencies between tokens that are far apart in the input sequence.\n\n## Subtext:\n\nThe attention mechanism has been shown to be effective in improving the performance of transformer models in various tasks, including machine translation, text classification, and question answering. However, it can also be challenging to interpret the attention weights, as they can be very large and difficult to visualize.\n\n## Subtext:\n\nIn this example, we can see that the attention weights are distributed across the input tokens, indicating that the model is able to capture dependencies between tokens that are far apart in the input sequence. The attention weights are also distributed across the input tokens, indicating that the model is able to capture dependencies between tokens that are far apart in the input sequence.\n\n## Subtext:\n\nThe attention mechanism has been shown to be effective in improving the performance of transformer models in various tasks, including machine translation, text classification, and question answering. However, it can also be challenging to interpret the attention weights, as they can be very large and difficult to visualize.\n\n## Subtext:\n\nIn this example, we can see that the attention weights are distributed across the input tokens, indicating that the model is able to capture dependencies between tokens that are far apart in the input sequence. The attention weights are also distributed across the input tokens, indicating that the model is able to capture dependencies between tokens that are far apart in the input sequence.\n\n## Subtext:\n\nThe attention mechanism has been shown to be effective in improving the performance of transformer models in various tasks, including machine translation, text classification, and question answering. However, it can also be challenging to interpret the attention weights, as they can be very large and difficult to visualize.\n\n## Subtext:\n\nIn this example, we can see that the attention weights are distributed across the input tokens, indicating that the model is able to capture dependencies between tokens that are far apart in the input sequence. The attention weights are also distributed across the input tokens, indicating that the model is able to capture dependencies between tokens that are far apart in the input sequence.\n\n## Subtext:\n\nThe attention mechanism has been shown to be effective in improving the performance of transformer models in various tasks, including machine translation, text classification, and question answering. However, it can also be challenging to interpret the attention weights, as they can be very large and difficult to visualize.\n\n## Subtext:\n\nIn this example, we can see that the attention weights are distributed across the input tokens, indicating that the model is able to capture dependencies between tokens that are far apart in the input sequence. The attention weights are also distributed across the input tokens, indicating that the model is able to capture dependencies between tokens that are far apart in the input sequence.\n\n## Subtext:\n\nThe attention mechanism has been shown to be effective in improving the performance of transformer models in various tasks, including machine translation, text classification, and question answering. However, it can also be challenging to interpret the attention weights, as they can be very large and difficult to visualize.\n\n## Subtext:\n\nIn this example, we can see that the attention weights are distributed across the input tokens, indicating that the model is able to capture dependencies between tokens that are far apart in the input sequence. The attention weights are displayed as a visualization.\n\n## Subtext:\n\nIn this example, we can see that the attention weights are distributed across the input tokens, indicating that the model is able to capture dependencies between tokens that the model is able to interpret the attention weights.\n\n## Subtext:\n\nIn this example, we can see that the attention weights are distributed across the input tokens, indicating that the model is able to interpret the attention weights.\n\n## Subtext:\n\nIn this example, we can see that the attention weights are distributed across the input tokens, indicating that that is a visualization.\n## Subtext:\nIn this example, indicating that the attention weights are visualization."
        },
        {
            "index": 13,
            "markdown": "# Page Title\n\n## Subtitle\n\n## Section 1\n\n## Section 2\n\n## Section 3\n\n## Section 4\n\n## Section 5\n\n## Section 6\n\n## Section 7\n\n## Section 8\n\n## Section 9\n\n## Section 10\n\n## Section 11\n\n## Section 12\n\n## Section 13\n\n## Section 14\n\n## Section 15\n\n## Section 16\n\n## Section 17\n\n## Section 18\n\n## Section 19\n\n## Section 20\n\n## Section 21\n\n## Section 22\n\n## Section 23\n\n## Section 24\n\n## Section 25\n\n## Section 26\n\n## Section 27\n\n## Section 28\n\n## Section 29\n\n## Section 30\n\n## Section 31\n\n## Section 32\n\n## Section 33\n\n## Section 34\n\n## Section 35\n\n## Section 36\n\n## Section 37\n\n## Section 38\n\n## Section 39\n\n## Section 40\n\n## Section 41\n\n## Section 42\n\n## Section 43\n\n## Section 44\n\n## Section 45\n\n## Section 46\n\n## Section 47\n\n## Section 48\n\n## Section 49\n\n## Section 50\n\n## Section 51\n\n## Section 52\n\n## Section 53\n\n## Section 54\n\n## Section 55\n\n## Section 56\n\n## Section 57\n\n## Section 58\n\n## Section 59\n\n## Section 60\n\n## Section 61\n\n## Section 62\n\n## Section 63\n\n## Section 64\n\n## Section 65\n\n## Section 66\n\n## Section 67\n\n## Section 68\n\n## Section 69\n\n## Section 70\n\n## Section 71\n\n## Section 72\n\n## Section 73\n\n## Section 74\n\n## Section 75\n\n## Section 76\n\n## Section 77\n\n## Section 78\n\n## Section 79\n\n## Section 80\n\n## Section 81\n\n## Section 82\n\n## Section 83\n\n## Section 84\n\n## Section 85\n\n## Section 86\n\n## Section 87\n\n## Section 88\n\n## Section 89\n\n## Section 90\n\n## Section 91\n\n## Section 92\n\n## Section 93\n\n## Section 94\n\n## Section 95\n\n## Section 96\n\n## Section 97\n\n## Section 98\n\n## Section 99\n\n## Section 100\n\n## Section 101\n\n## Section 102\n\n## Section 103\n\n## Section 104\n\n## Section 105\n\n## Section 106\n\n## Section 107\n\n## Section 108\n\n## Section 109\n\n## Section 110\n\n## Section 111\n\n## Section 112\n\n## Section 113\n\n## Section 114\n\n## Section 115\n\n## Section 116\n\n## Section 117\n\n## Section 118\n\n## Section 119\n\n## Section 120\n\n## Section 121\n\n## Section 122\n\n## Section 123\n\n## Section 124\n\n## Section 125\n\n## Section 126\n\n## Section 127\n\n## Section 128\n\n## Section 129\n\n## Section 130\n\n## Section 131\n\n## Section 132\n\n## Section 133\n\n## Section 134\n\n## Section 135\n\n## Section 136\n\n## Section 137\n\n## Section 138\n\n## Section 139\n\n## Section 140\n\n## Section 141\n\n## Section 142\n\n## Section 143\n\n## Section 144\n\n## Section 145\n\n## Section 146\n\n## Section 147\n\n## Section 148\n\n## Section 149\n\n## Section 150\n\n## Section 151\n\n## Section 152\n\n## Section 153\n\n## Section 154\n\n## Section 155\n\n## Section 156\n\n## Section 157\n\n## Section 158\n\n## Section 159\n\n## Section 160\n\n## Section 161\n\n## Section 162\n\n## Section 163\n\n## Section 164\n\n## Section 165\n\n## Section 166\n\n## Section 167\n\n## Section 168\n\n## Section 169\n\n## Section 170\n\n## Section 171\n\n## Section 172\n\n## Section 173\n\n## Section 174\n\n## Section 175\n\n## Section 176\n\n## Section 177\n\n## Section 178\n\n## Section 179\n\n## Section 180\n\n## Section 181\n\n## Section 182\n\n## Section 183\n\n## Section 184\n\n## Section 185\n\n## Section 186\n\n## Section 187\n\n## Section 188\n\n## Section 189\n\n## Section 190\n\n## Section 191\n\n## Section 192\n\n## Section 193\n\n## Section 194\n\n## Section 195\n\n## Section 196\n\n## Section 197\n\n## Section 198\n\n## Section 199\n\n## Section 200\n\n## Section 201\n\n## Section 202\n\n## Section 203\n\n## Section 204\n\n## Section 205\n\n## Section 206\n\n## Section 207\n\n## Section 208\n\n## Section 209\n\n## Section 210\n\n## Section 211\n\n## Section 212\n\n## Section 213\n\n## Section 214\n\n## Section 215\n\n## Section 216\n\n## Section 217\n\n## Section 218\n\n## Section 219\n\n## Section 220\n\n## Section 221\n\n## Section 222\n\n## Section 223\n\n## Section 224\n\n## Section 225\n\n## Section 226\n\n## Section 227\n\n## Section 228\n\n## Section 229\n\n## Section 230\n\n## Section 231\n\n## Section 232\n\n## Section 233\n\n## Section 234\n\n## Section 235\n\n## Section 236\n\n## Section 237\n\n## Section 238\n\n## Section 239\n\n## Section 240\n\n## Section 241\n\n## Section 242\n\n## Section 243\n\n## Section 244\n\n## Section 245\n\n## Section 246\n\n## Section 247\n\n## Section 248\n\n## Section 249\n\n## Section 250\n\n## Section 251\n\n## Section 252\n\n## Section 253\n\n## Section 254\n\n## Section 255\n\n## Section 256\n\n## Section 257\n\n## Section 258\n\n## Section 259\n\n## Section 260\n\n## Section 261\n\n## Section 262\n\n## Section 263\n\n## Section 264\n\n## Section 265\n\n## Section 266\n\n## Section 267\n\n## Section 268\n\n## Section 269\n\n## Section 270\n\n## Section 271\n\n## Section 272\n\n## Section 273\n\n## Section 274\n\n## Section 275\n\n## Section 276\n\n## Section 277\n\n## Section 278\n\n## Section 279\n\n## Section 280\n\n## Section 281\n\n## Section 282\n\n## Section 283\n\n## Section 284\n\n## Section 285\n\n## Section 286\n\n## Section 287\n\n## Section 288\n\n## Section 289\n\n## Section 290\n\n## Section 291\n\n## Section 292\n\n## Section 293\n\n## Section 294\n\n## Section 295\n\n## Section 296\n\n## Section 297\n\n## Section 298\n\n## Section 299\n\n## Section 300\n\n## Section 301\n\n## Section 302\n\n## Section 303\n\n## Section 304\n\n## Section 305\n\n## Section 306\n\n## Section 307\n\n## Section 308\n\n## Section 309\n\n## Section 310\n\n## Section 311\n\n## Section 312\n\n## Section 313\n\n## Section 314\n\n## Section 315\n\n## Section 316\n\n## Section 317\n\n## Section 318\n\n## Section 319\n\n## Section 320\n\n## Section 321\n\n## Section 322\n\n## Section 323\n\n## Section 324\n\n## Section 325\n\n## Section 326\n\n## Section 327\n\n## Section 328\n\n## Section 329\n\n## Section 330\n\n## Section 331\n\n## Section 332\n\n## Section 333\n\n## Section 334\n\n## Section 335\n\n## Section 336\n\n## Section 337\n\n## Section 338\n\n## Section 339\n\n## Section 340\n\n## Section 341\n\n## Section 342\n\n## Section 343\n\n## Section 344\n\n## Section 345\n\n## Section 346\n\n## Section 347\n\n## Section 348\n\n## Section 349\n\n## Section 350\n\n## Section 351\n\n## Section 352\n\n## Section 353\n\n## Section 354\n\n## Section 355\n\n## Section 356\n\n## Section 357\n\n## Section 358\n\n## Section 359\n\n## Section 360\n\n## Section 361\n\n## Section 362\n\n## Section 363\n\n## Section 364\n\n## Section 365\n\n## Section 366\n\n## Section 367\n\n## Section 368\n\n## Section 369\n\n## Section 370\n\n## Section 371\n\n## Section 372\n\n## Section 373\n\n## Section 374\n\n## Section 375\n\n## Section 376\n\n## Section 377\n\n## Section 378\n\n## Section 379\n\n## Section 380\n\n## Section 381\n\n## Section 382\n\n## Section 383\n\n## Section 384\n\n## Section 385\n\n## Section 386\n\n## Section 387\n\n## Section 388\n\n## Section 389\n\n## Section 390\n\n## Section 391\n\n## Section 392\n\n## Section 393\n\n## Section 394\n\n## Section 395\n\n## Section 396\n\n## Section 397\n\n## Section 398\n\n## Section 399\n\n## Section 400\n\n## Section 401\n\n## Section 402\n\n## Section 403\n\n## Section 404\n\n## Section 405\n\n## Section 406\n\n## Section 407\n\n## Section 408\n\n## Section 409\n\n## Section 410\n\n## Section 411\n\n## Section 412\n\n## Section 413\n\n## Section 414\n\n## Section 415\n\n## Section 416\n\n## Section 417\n\n## Section 418\n\n## Section 419\n\n## Section 420\n\n## Section 421\n\n## Section 422\n\n## Section 423\n\n## Section 424\n\n## Section 425\n\n## Section 426\n\n## Section 427\n\n## Section 428\n\n## Section 429\n\n## Section 430\n\n## Section 431\n\n## Section 432\n\n## Section 433\n\n## Section 434\n\n## Section 435\n\n## Section 436\n\n## Section 437\n\n## Section 438\n\n## Section 439\n\n## Section 440\n\n## Section 441\n\n## Section 442\n\n## Section 443\n\n## Section 444\n\n## Section 445\n\n## Section 446\n\n## Section 447\n\n## Section 448\n\n## Section 449\n\n## Section 450\n\n## Section 451\n\n## Section 452\n\n## Section 453\n\n## Section 454\n## Section 455\n## Section 456\n\n## Section 457\n## Section 460\n## Section 460\n## Section 461\n## Section 462\n## Section 463\n## Section 464\n## Section 465\n## Section 466\n## Section 467\n## 468\n## Section 469\n## 469\n## 469\n## 469\n## 469\n## 469\n## 469\n## 469\n## 469\n## 469\n## 469\n##\ufffd969999999\n##."
        },
        {
            "index": 14,
            "markdown": "# Figure 2: Many of the attention heads exhibit behavior that seems related to the structure of the input.\n\n## Subtext:\nFigure 2 shows the attention heads that exhibit behavior that seems related to the structure of the input. The results are similar to those of previous studies.\n\n## Figure 2:\n\n![Figure 2](https://example.com/figure-2.png)\n\n## Subtext:\nThe results clearly show the attention heads learned to perform different tasks.\n"
        }
    ]
}